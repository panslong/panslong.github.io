import{_ as i}from"./plugin-vue_export-helper-c27b6911.js";import{o as l,c as e,d as p}from"./app-de394487.js";const a="/assets/1653844970346-79ce971d.png",t="/assets/1653845147190-e70689a4.png",s="/assets/1653896065386-5f2d9e94.png",o="/assets/1653896377259-aa8fe1c0.png",r="/assets/1653896687354-92c69283.png",n="/assets/1653897115346-13387280.png",d="/assets/1653897270074-70508d0e.png",c="/assets/1653897490116-cef1f821.png",g="/assets/1653898691736-595e7808.png",f="/assets/1653900022580-36094a7b.png",u="/assets/1653900721427-e8307533.png",_="/assets/20231110115243-bb94656a.png",h="/assets/1653902845082-ebaca184.png",I="/assets/1653911776583-6282afd4.png",m="/assets/1653911877542-457f8c56.png",O="/assets/1653912219712-4bd9bafe.png",b="/assets/20231110160327-a5d99aca.png",x="/assets/1653982278727-48ee8000.png",D="/assets/20231110162945-56780308.png",F={},y=p('<h3 id="_1-用户空间和内核态空间" tabindex="-1"><a class="header-anchor" href="#_1-用户空间和内核态空间" aria-hidden="true">#</a> 1. 用户空间和内核态空间</h3><p>服务器大多都采用Linux系统，这里我们以Linux为例来讲解:</p><p>ubuntu和Centos 都是Linux的发行版，发行版可以看成对linux包了一层壳，任何Linux发行版，其系统内核都是Linux。我们的应用都需要通过Linux内核与硬件交互</p><figure><img src="'+a+'" alt="1653844970346" tabindex="0" loading="lazy"><figcaption>1653844970346</figcaption></figure><p>用户的应用，比如redis，mysql等其实是没有办法去执行访问我们操作系统的硬件的，所以我们可以通过发行版的这个壳子去访问内核，再通过内核去访问计算机硬件</p><figure><img src="'+t+'" alt="1653845147190" tabindex="0" loading="lazy"><figcaption>1653845147190</figcaption></figure><p>计算机硬件包括，如cpu，内存，网卡等等，内核（通过寻址空间）可以操作硬件的，但是内核需要不同设备的驱动，有了这些驱动之后，内核就可以去对计算机硬件去进行 内存管理，文件系统的管理，进程的管理等等</p><figure><img src="'+s+'" alt="1653896065386" tabindex="0" loading="lazy"><figcaption>1653896065386</figcaption></figure><p>我们想要用户的应用来访问，计算机就必须要通过对外暴露的一些接口，才能访问到，从而简介的实现对内核的操控，但是内核本身上来说也是一个应用，所以他本身也需要一些内存，cpu等设备资源，用户应用本身也在消耗这些资源，如果不加任何限制，用户去操作随意的去操作我们的资源，就有可能导致一些冲突，甚至有可能导致我们的系统出现无法运行的问题，因此我们需要把用户和<strong>内核隔离开</strong></p><p>进程的寻址空间划分成两部分：<strong>内核空间、用户空间</strong></p><p>什么是寻址空间呢？我们的应用程序也好，还是内核空间也好，都是没有办法直接去物理内存的，而是通过分配一些虚拟内存映射到物理内存中，我们的内核和应用程序去访问虚拟内存的时候，就需要一个虚拟地址，这个地址是一个无符号的整数，比如一个32位的操作系统，他的带宽就是32，他的虚拟地址就是2的32次方，也就是说他寻址的范围就是0~2的32次方， 这片寻址空间对应的就是2的32个字节，就是4GB，这个4GB，会有3个GB分给用户空间，会有1GB给内核系统</p><figure><img src="'+o+'" alt="1653896377259" tabindex="0" loading="lazy"><figcaption>1653896377259</figcaption></figure><p>在linux中，他们权限分成两个等级，0和3，用户空间只能执行受限的命令（Ring3），而且不能直接调用系统资源，必须通过内核提供的接口来访问内核空间可以执行特权命令（Ring0），调用一切系统资源，所以一般情况下，用户的操作是运行在用户空间，而内核运行的数据是在内核空间的，而有的情况下，一个应用程序需要去调用一些特权资源，去调用一些内核空间的操作，所以此时他俩需要在用户态和内核态之间进行切换。</p><p>比如：</p><p>Linux系统为了提高IO效率，会在用户空间和内核空间都加入缓冲区：</p><p>写数据时，要把用户缓冲数据拷贝到内核缓冲区，然后写入设备</p><p>读数据时，要从设备读取数据到内核缓冲区，然后拷贝到用户缓冲区</p><p>针对这个操作：我们的用户在写读数据时，会去向内核态申请，想要读取内核的数据，而内核数据要去等待驱动程序从硬件上读取数据，当从磁盘上加载到数据之后，内核会将数据写入到内核的缓冲区中，然后再将数据拷贝到用户态的buffer中，然后再返回给应用程序，整体而言，速度慢，就是这个原因，为了加速，我们希望read也好，还是wait for data也最好都不要等待，或者时间尽量的短。</p><figure><img src="'+r+'" alt="1653896687354" tabindex="0" loading="lazy"><figcaption>1653896687354</figcaption></figure><h3 id="_2-网络模型-阻塞io" tabindex="-1"><a class="header-anchor" href="#_2-网络模型-阻塞io" aria-hidden="true">#</a> 2.网络模型-阻塞IO</h3><p>在《UNIX网络编程》一书中，总结归纳了5种IO模型：</p><ul><li>阻塞IO（Blocking IO）</li><li>非阻塞IO（Nonblocking IO）</li><li>IO多路复用（IO Multiplexing）</li><li>信号驱动IO（Signal Driven IO）</li><li>异步IO（Asynchronous IO）</li></ul><p>应用程序想要去读取数据，他是无法直接去读取磁盘数据的，他需要先到内核里边去等待内核操作硬件拿到数据，这个过程就是1，是需要等待的，等到内核从磁盘上把数据加载出来之后，再把这个数据写给用户的缓存区，这个过程是2，如果是阻塞IO，那么整个过程中，用户从发起读请求开始，一直到读取到数据，都是一个阻塞状态。</p><figure><img src="'+n+'" alt="1653897115346" tabindex="0" loading="lazy"><figcaption>1653897115346</figcaption></figure><p>具体流程如下图：</p><p>用户去读取数据时，会去先发起recvform一个命令，去尝试从内核上加载数据，如果内核没有数据，那么用户就会等待，此时内核会去从硬件上读取数据，内核读取数据之后，会把数据拷贝到用户态，并且返回ok，整个过程，都是阻塞等待的，这就是阻塞IO</p><p>总结如下：</p><p>顾名思义，阻塞IO就是两个阶段都必须阻塞等待：</p><p><strong>阶段一：</strong></p><ul><li>用户进程尝试读取数据（比如网卡数据）</li><li>此时数据尚未到达，内核需要等待数据</li><li>此时用户进程也处于阻塞状态</li></ul><p>阶段二：</p><ul><li>数据到达并拷贝到内核缓冲区，代表已就绪</li><li>将内核数据拷贝到用户缓冲区</li><li>拷贝过程中，用户进程依然阻塞等待</li><li>拷贝完成，用户进程解除阻塞，处理数据</li></ul><p>可以看到，阻塞IO模型中，用户进程在两个阶段都是阻塞状态。</p><figure><img src="'+d+'" alt="1653897270074" tabindex="0" loading="lazy"><figcaption>1653897270074</figcaption></figure><h3 id="_3-网络模型-非阻塞io" tabindex="-1"><a class="header-anchor" href="#_3-网络模型-非阻塞io" aria-hidden="true">#</a> 3. 网络模型-非阻塞IO</h3><p>顾名思义，非阻塞IO的recvfrom操作会立即返回结果而不是阻塞用户进程。</p><p>阶段一：</p><ul><li>用户进程尝试读取数据（比如网卡数据）</li><li>此时数据尚未到达，内核需要等待数据</li><li>返回异常给用户进程</li><li>用户进程拿到error后，再次尝试读取</li><li>循环往复，直到数据就绪</li></ul><p>阶段二：</p><ul><li>将内核数据拷贝到用户缓冲区</li><li>拷贝过程中，用户进程依然阻塞等待</li><li>拷贝完成，用户进程解除阻塞，处理数据</li><li>可以看到，非阻塞IO模型中，用户进程在第一个阶段是非阻塞，第二个阶段是阻塞状态。虽然是非阻塞，但性能并没有得到提高。而且忙等机制会导致CPU空转，CPU使用率暴增。</li></ul><figure><img src="'+c+'" alt="1653897490116" tabindex="0" loading="lazy"><figcaption>1653897490116</figcaption></figure><h3 id="_4-网络模型-io多路复用" tabindex="-1"><a class="header-anchor" href="#_4-网络模型-io多路复用" aria-hidden="true">#</a> 4. 网络模型-IO多路复用</h3><p>无论是阻塞IO还是非阻塞IO，用户应用在一阶段都需要调用recvfrom来获取数据，差别在于无数据时的处理方案：</p><p>如果调用recvfrom时，恰好没有数据，阻塞IO会使CPU阻塞，非阻塞IO使CPU空转，都不能充分发挥CPU的作用。<br> 如果调用recvfrom时，恰好有数据，则用户进程可以直接进入第二阶段，读取并处理数据</p><p>所以怎么看起来以上两种方式性能都不好</p><p>而在单线程情况下，只能依次处理IO事件，如果正在处理的IO事件恰好未就绪（数据不可读或不可写），线程就会被阻塞，所有IO事件都必须等待，性能自然会很差。</p><p>就比如服务员给顾客点餐，<strong>分两步</strong>：</p><ul><li>顾客思考要吃什么（等待数据就绪）</li><li>顾客想好了，开始点餐（读取数据）</li></ul><p>要提高效率有几种办法？</p><p>方案一：增加更多服务员（多线程）<br> 方案二：不排队，谁想好了吃什么（数据就绪了），服务员就给谁点餐（用户应用就去读取数据）</p><p>那么问题来了：用户进程如何知道内核中数据是否就绪呢？</p><p>所以接下来就需要详细的来解决多路复用模型是如何知道到底怎么知道内核数据是否就绪的问题了</p><p>这个问题的解决依赖于提出的</p><p>文件描述符（File Descriptor）：简称FD，是一个从0 开始的无符号整数，用来关联Linux中的一个文件。在Linux中，一切皆文件，例如常规文件、视频、硬件设备等，当然也包括网络套接字（Socket）。</p><p>通过FD，我们的网络模型可以利用一个线程监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。</p><p>阶段一：</p><ul><li>用户进程调用select，指定要监听的FD集合</li><li>核监听FD对应的多个socket</li><li>任意一个或多个socket数据就绪则返回readable</li><li>此过程中用户进程阻塞</li></ul><p>阶段二：</p><ul><li>用户进程找到就绪的socket</li><li>依次调用recvfrom读取数据</li><li>内核将数据拷贝到用户空间</li><li>用户进程处理数据</li></ul><p>当用户去读取数据的时候，不再去直接调用recvfrom了，而是调用select的函数，select函数会将需要监听的数据交给内核，由内核去检查这些数据是否就绪了，如果说这个数据就绪了，就会通知应用程序数据就绪，然后来读取数据，再从内核中把数据拷贝给用户态，完成数据处理，如果N多个FD一个都没处理完，此时就进行等待。</p><p>用IO复用模式，可以确保去读数据的时候，数据是一定存在的，他的效率比原来的阻塞IO和非阻塞IO性能都要高</p><figure><img src="'+g+'" alt="1653898691736" tabindex="0" loading="lazy"><figcaption>1653898691736</figcaption></figure><p>IO多路复用是利用单个线程来同时监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。不过监听FD的方式、通知的方式又有多种实现，常见的有：</p><ul><li>select</li><li>poll</li><li>epoll</li></ul><p>其中select和pool相当于是当被监听的数据准备好之后，他会把你监听的FD整个数据都发给你，你需要到整个FD中去找，哪些是处理好了的，需要通过遍历的方式，所以性能也并不是那么好</p><p>而epoll，则相当于内核准备好了之后，他会把准备好的数据，直接发给你，咱们就省去了遍历的动作。</p><h3 id="_5-网络模型-io多路复用-select方式" tabindex="-1"><a class="header-anchor" href="#_5-网络模型-io多路复用-select方式" aria-hidden="true">#</a> 5. 网络模型-IO多路复用-select方式</h3><p>select是Linux最早是由的I/O多路复用技术：</p><p>简单说，就是我们把需要处理的数据封装成FD，然后在用户态时创建一个fd的集合（这个集合的大小是要监听的那个FD的最大值+1，但是大小整体是有限制的 ），这个集合的长度大小是有限制的，同时在这个集合中，标明出来我们要控制哪些数据，</p><p>比如要监听的数据，是1,2,5三个数据，此时会执行select函数，然后将整个fd发给内核态，内核态会去遍历用户态传递过来的数据，如果发现这里边都数据都没有就绪，就休眠，直到有数据准备好时，就会被唤醒，唤醒之后，再次遍历一遍，看看谁准备好了，然后再将处理掉没有准备好的数据，最后再将这个FD集合写回到用户态中去，此时用户态就知道了，奥，有人准备好了，但是对于用户态而言，并不知道谁处理好了，所以用户态也需要去进行遍历，然后找到对应准备好数据的节点，再去发起读请求，我们会发现，这种模式下他虽然比阻塞IO和非阻塞IO好，但是依然有些麻烦的事情， 比如说频繁的传递fd集合，频繁的去遍历FD等问题</p><figure><img src="'+f+'" alt="1653900022580" tabindex="0" loading="lazy"><figcaption>1653900022580</figcaption></figure><p>模式存在的问题：</p><ul><li>需要将整个fd_set从用户空间拷贝到内核空间，select结束还要再次拷贝回用户空间</li><li>select无法得知具体是哪个fd就绪，需要遍历整个fd_set</li><li>fd_set监听的fd数量不能超过1024</li></ul><h3 id="_6-网络模型-io多路复用模型-poll模式" tabindex="-1"><a class="header-anchor" href="#_6-网络模型-io多路复用模型-poll模式" aria-hidden="true">#</a> 6. 网络模型-IO多路复用模型-poll模式</h3><p>poll模式对select模式做了简单改进，但性能提升不明显，部分关键代码如下：</p><p>IO流程：</p><ul><li>创建pollfd数组，向其中添加关注的fd信息，数组大小自定义</li><li>调用poll函数，将pollfd数组拷贝到内核空间，转链表存储，无上限</li><li>内核遍历fd，判断是否就绪</li><li>数据就绪或超时后，拷贝pollfd数组到用户空间，返回就绪fd数量n</li><li>用户进程判断n是否大于0,大于0则遍历pollfd数组，找到就绪的fd</li></ul><p><strong>与select对比：</strong></p><ul><li>select模式中的fd_set大小固定为1024，而pollfd在内核中采用链表，理论上无上限</li><li>监听FD越多，每次遍历消耗时间也越久，性能反而会下降</li></ul><figure><img src="'+u+'" alt="1653900721427" tabindex="0" loading="lazy"><figcaption>1653900721427</figcaption></figure><h3 id="_7-网络模型-io多路复用模型-epoll函数" tabindex="-1"><a class="header-anchor" href="#_7-网络模型-io多路复用模型-epoll函数" aria-hidden="true">#</a> 7. 网络模型-IO多路复用模型-epoll函数</h3><p>epoll模式是对select和poll的改进，它提供了三个函数：</p><p>第一个是：eventpoll的函数，他内部包含两个东西</p><p>一个是：</p><p>1、红黑树-&gt; 记录的事要监听的FD</p><p>2、一个是链表-&gt;一个链表，记录的是就绪的FD</p><p>紧接着调用epoll_ctl操作，将要监听的数据添加到红黑树上去，并且给每个fd设置一个监听函数，这个函数会在fd数据就绪时触发，就是准备好了，现在就把fd把数据添加到list_head中去</p><p>3、调用epoll_wait函数</p><p>就去等待，在用户态创建一个空的events数组，当就绪之后，我们的回调函数会把数据添加到list_head中去，当调用这个函数的时候，会去检查list_head，当然这个过程需要参考配置的等待时间，可以等一定时间，也可以一直等， 如果在此过程中，检查到了list_head中有数据会将数据添加到链表中，此时将数据放入到events数组中，并且返回对应的操作的数量，用户态的此时收到响应后，从events中拿到对应准备好的数据的节点，再去调用方法去拿数据。</p><p>小总结：</p><p>select模式存在的三个问题：</p><ul><li>能监听的FD最大不超过1024</li><li>每次select都需要把所有要监听的FD都拷贝到内核空间</li><li>每次都要遍历所有FD来判断就绪状态</li></ul><p>poll模式的问题：</p><ul><li>poll利用链表解决了select中监听FD上限的问题，但依然要遍历所有FD，如果监听较多，性能会下降</li></ul><p>epoll模式中如何解决这些问题的？</p><ul><li>基于epoll实例中的红黑树保存要监听的FD，理论上无上限，而且增删改查效率都非常高</li><li>每个FD只需要执行一次epoll_ctl添加到红黑树，以后每次epol_wait无需传递任何参数，无需重复拷贝FD到内核空间</li><li>利用ep_poll_callback机制来监听FD状态，无需遍历所有FD，因此性能不会随监听的FD数量增多而下降</li></ul><figure><img src="'+_+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_8-网络模型-epoll中的et和lt" tabindex="-1"><a class="header-anchor" href="#_8-网络模型-epoll中的et和lt" aria-hidden="true">#</a> 8. 网络模型-epoll中的ET和LT</h3><p>当FD有数据可读时，我们调用epoll_wait（或者select、poll）可以得到通知。但是事件通知的模式有两种：</p><ul><li>LevelTriggered：简称LT，也叫做水平触发。只要某个FD中有数据可读，每次调用epoll_wait都会得到通知。</li><li>EdgeTriggered：简称ET，也叫做边沿触发。只有在某个FD有状态变化时，调用epoll_wait才会被通知。</li></ul><p>举个栗子：</p><ul><li>假设一个客户端socket对应的FD已经注册到了epoll实例中</li><li>客户端socket发送了2kb的数据</li><li>服务端调用epoll_wait，得到通知说FD就绪</li><li>服务端从FD读取了1kb数据回到步骤3（再次调用epoll_wait，形成循环）</li></ul><p>结论</p><p>如果我们采用LT模式，因为FD中仍有1kb数据，则第⑤步依然会返回结果，并且得到通知<br> 如果我们采用ET模式，因为第③步已经消费了FD可读事件，第⑤步FD状态没有变化，因此epoll_wait不会返回，数据无法读取，客户端响应超时。</p><p>LT：事件通知频率较高，会有重复通知，影响性能</p><p>ET：仅通知一次，效率高。可以基于非阻塞IO循环读取解决数据读取不完整问题</p><p>select和poll仅支持LT模式，epoll可以自由选择LT和ET两种模式</p><h3 id="_9-网络模型-基于epoll的服务器端流程" tabindex="-1"><a class="header-anchor" href="#_9-网络模型-基于epoll的服务器端流程" aria-hidden="true">#</a> 9. 网络模型-基于epoll的服务器端流程</h3><p>我们来梳理一下这张图</p><p>服务器启动以后，服务端会去调用epoll_create，创建一个epoll实例，epoll实例中包含两个数据</p><p>1、红黑树（为空）：rb_root 用来去记录需要被监听的FD</p><p>2、链表（为空）：list_head，用来存放已经就绪的FD</p><p>创建好了之后，会去调用epoll_ctl函数，此函数会会将需要监听的数据添加到rb_root中去，并且对当前这些存在于红黑树的节点设置回调函数，当这些被监听的数据一旦准备完成，就会被调用，而调用的结果就是将红黑树的fd添加到list_head中去(但是此时并没有完成)</p><p>3、当第二步完成后，就会调用epoll_wait函数，这个函数会去校验是否有数据准备完毕（因为数据一旦准备就绪，就会被回调函数添加到list_head中），在等待了一段时间后(可以进行配置)，如果等够了超时时间，则返回没有数据，如果有，则进一步判断当前是什么事件，如果是建立连接时间，则调用accept() 接受客户端socket，拿到建立连接的socket，然后建立起来连接，如果是其他事件，则把数据进行写出</p><figure><img src="'+h+'" alt="1653902845082" tabindex="0" loading="lazy"><figcaption>1653902845082</figcaption></figure><h3 id="_10-、网络模型-信号驱动io" tabindex="-1"><a class="header-anchor" href="#_10-、网络模型-信号驱动io" aria-hidden="true">#</a> 10 、网络模型-信号驱动IO</h3><p>信号驱动IO是与内核建立SIGIO的信号关联并设置回调，当内核有FD就绪时，会发出SIGIO信号通知用户，期间用户应用可以执行其它业务，无需阻塞等待。</p><p>阶段一：</p><ul><li>用户进程调用sigaction，注册信号处理函数</li><li>内核返回成功，开始监听FD</li><li>用户进程不阻塞等待，可以执行其它业务</li><li>当内核数据就绪后，回调用户进程的SIGIO处理函数</li></ul><p>阶段二：</p><ul><li>收到SIGIO回调信号</li><li>调用recvfrom，读取</li><li>内核将数据拷贝到用户空间</li><li>用户进程处理数据</li></ul><figure><img src="'+I+'" alt="1653911776583" tabindex="0" loading="lazy"><figcaption>1653911776583</figcaption></figure><p>缺点：当有大量IO操作时，信号较多，SIGIO处理函数不能及时处理可能导致信号队列溢出，而且内核空间与用户空间的频繁信号交互性能也较低。</p><h3 id="_11-异步io" tabindex="-1"><a class="header-anchor" href="#_11-异步io" aria-hidden="true">#</a> 11. 异步IO</h3><p>异步IO的整个过程都是非阻塞的，用户进程调用完异步API后就可以去做其它事情，内核等待数据就绪并拷贝到用户空间后才会递交信号，通知用户进程。</p><p>阶段一：</p><p>1.用户进程调用aio_read，创建信号回调函数</p><p>2.内核等待数据就绪</p><p>3.用户进程无需阻塞，可以做任何事情</p><p>阶段二：</p><p>1.内核数据就绪</p><p>2.内核数据拷贝到用户缓冲区</p><p>3.拷贝完成，内核递交信号触发aio_read中的回调函数</p><p>4.用户进程处理数据</p><p>可以看到，异步IO模型中，用户进程在两个阶段都是非阻塞状态。</p><p>他会由内核将所有数据处理完成后，由内核将数据写入到用户态中，然后才算完成，所以性能极高，不会有任何阻塞，全部都由内核完成，可以看到，异步IO模型中，用户进程在两个阶段都是非阻塞状态。</p><figure><img src="'+m+'" alt="1653911877542" tabindex="0" loading="lazy"><figcaption>1653911877542</figcaption></figure><h3 id="_12-对比" tabindex="-1"><a class="header-anchor" href="#_12-对比" aria-hidden="true">#</a> 12. 对比</h3><p>最后用一幅图，来说明他们之间的区别</p><figure><img src="'+O+'" alt="1653912219712" tabindex="0" loading="lazy"><figcaption>1653912219712</figcaption></figure><h3 id="_13-网络模型-redis是单线程的吗-为什么使用单线程" tabindex="-1"><a class="header-anchor" href="#_13-网络模型-redis是单线程的吗-为什么使用单线程" aria-hidden="true">#</a> 13 . 网络模型-Redis是单线程的吗？为什么使用单线程</h3><p><strong>Redis到底是单线程还是多线程？</strong></p><ul><li>如果仅仅聊Redis的核心业务部分（命令处理），答案是单线程</li><li>如果是聊整个Redis，那么答案就是多线程</li></ul><p>在Redis版本迭代过程中，在两个重要的时间节点上引入了多线程的支持：</p><ul><li>Redis v4.0：引入多线程异步处理一些耗时较旧的任务，例如异步删除命令unlink</li><li>Redis v6.0：在核心网络模型中引入 多线程，进一步提高对于多核CPU的利用率</li></ul><p>因此，对于Redis的核心网络模型，在Redis 6.0之前确实都是单线程。是利用epoll（Linux系统）这样的IO多路复用技术在事件循环中不断处理客户端情况。</p><p><strong>为什么Redis要选择单线程？</strong></p><ul><li>抛开持久化不谈，Redis是纯 内存操作，执行速度非常快，它的性能瓶颈是网络延迟而不是执行速度，因此多线程并不会带来巨大的性能提升。</li><li>多线程会导致过多的上下文切换，带来不必要的开销</li><li>引入多线程会面临线程安全问题，必然要引入线程锁这样的安全手段，实现复杂度增高，而且性能也会大打折扣</li></ul><h3 id="_14-redis的单线程模型-redis单线程和多线程网络模型变更" tabindex="-1"><a class="header-anchor" href="#_14-redis的单线程模型-redis单线程和多线程网络模型变更" aria-hidden="true">#</a> 14 . Redis的单线程模型-Redis单线程和多线程网络模型变更</h3><p>Redis通过IO多路复用来提高网络性能，并且支持各种不同的多路复用实现，并且将这些实现进行封装， 提供了统一的高性能事件库API库 AE：</p><figure><img src="'+b+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+x+'" alt="1653982278727" tabindex="0" loading="lazy"><figcaption>1653982278727</figcaption></figure><p>当我们的客户端想要去连接我们服务器，会去先到IO多路复用模型去进行排队，会有一个连接应答处理器，他会去接受读请求，然后又把读请求注册到具体模型中去，此时这些建立起来的连接，如果是客户端请求处理器去进行执行命令时，他会去把数据读取出来，然后把数据放入到client中， client去解析当前的命令转化为redis认识的命令，接下来就开始处理这些命令，从redis中的command中找到这些命令，然后就真正的去操作对应的数据了，当数据操作完成后，会去找到命令回复处理器，再由他将数据写出。</p><p>Redis 6.0版本中引入了多线程，目的是为了提高IO读写效率。因此在解析客户端命令、写响应结果时采用了多线程。核心的命令执行、IO多路复用模块依然是由主线程执行。</p><figure><img src="'+D+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',155),z=[y];function R(k,v){return l(),e("div",null,z)}const w=i(F,[["render",R],["__file","2023-11-10-Redis-原理-网络模型.html.vue"]]);export{w as default};
